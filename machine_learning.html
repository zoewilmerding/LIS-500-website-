<!DOCTYPE html>
<html>
<head> 
    <title>Machine Learning</title>

    <!-- Link to external stylesheet -->
    <link rel="stylesheet" text="text/css" href="stylesheet2.css">

    <!-- Google Fonts: Playfair Display -->
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@500;700&display=swap" rel="stylesheet">
</head>

<body>
    <!-- HEADER SECTION -->
    <header> 
        <h1>Machine Learning</h1>

        <!-- Navigation Menu -->
        <nav> 
            <ul>
                <li><a href="index.html">Home</a></li> 
                <li><a href="about.html">About us</a></li> 
                <li><a href="resources.html">Resources</a></li>
                <li><a href="tech_hero.html">Tech Hero</a></li>
                <li><a href="machine_learning.html">Machine Learning</a></li>
            </ul>
        </nav>
    </header>

    <!-- MAIN CONTENT SECTION -->
    <section>
        <main>
            <article class="mart">

                <!-- PROJECT STATEMENT HEADER -->
                <h2 id="title">Project Statement</h2>

                <!-- Project Statement Paragraphs -->
               <p> Our goal for the machine learning project was to understand first-hand how machine learning models are built, trained, and how design choices impact the accuracy of the systems. To do this, we created a teachable machine qualifier that identified between three basic facial expressions: happy, neutral, and sad. Through the design process, we were constantly pulling themes from this semester's reading of Unmasking AI, focusing on the importance of ethical decision-making. </p> 
                <p> After running into issues with the machine classifying the "neutral" emotion, we decided to incorporate clear, visible hand signals. This addition simplified the recognition task and showed us the importance of detail for machine learning to interpret human behavior. At first, we decided to have Amanda take all the pictures for each of the three emotions so the facial expressions and hand signals were as consistent as possible across the hundred photos. This kept the lighting, angle, distance from the camera, and camera quality all the same across the images. We quickly learned that the more photos we had of each expression, the easier it was for the machine to correctly identify the emotion. </p> 
                <p>After receiving feedback on our model, we realized there were many issues with having Amanda take every photo. There were problems with the model because all of the images came from Amanda, and the system learned to only recognize the emotions expressed through her face, skin tone, hairstyle, lighting, and camera angle. These controlled conditions made the training easier as only one person needed to take the photos for the model, but it performed worse when others tried it. Thus, tying back to Boulawini's argument that when data lacks diversity, the system performs the best on the people it is most familiar with. To fix this problem, we had each group member and even some friends add images. Inducing multiple faces, hair colors, lighting conditions, and expression styles improved our model significantly. While our original model seemed to be working well when Amanda was the user when put into a real-world variation, it lacked accuracy. We found that the more people we got to take images for the model, the better it worked. The improvements between our models showed us that representation matters and the choices behind data are powerful. </p> 
                <p> We repeated the design cycle of collect, train, test, and adjust over and over until the machine was working just how we liked it. As a group, we made conscious decisions about which images to keep and made sure that the images reflected the emotions we were trying to portray. These decisions we made as a group shaped how the machine learns to interpret human emotion. </p> 
                <p> When designing our model, we quickly found that a neutral expression was nearly impossible for the machine to accurately identify. We found that it was hard for us all to have the same level of neutrality for the machine to recognize correctly. For every participant, some shots looked more serious, leaned towards happy, or appeared as sad. With practice of taking many photos and adding the hand motion, we were able to overcome this issue. We were constantly referring back to Buolamwi's core themes. As she explains in her book, AI systems take on the subjective assumptions in their training. Our model reinforced this argument that these teachable machines reflect the interpretations of the ones creating them. </p> 
                <p> The process of creating the model made us extremely aware of the ethical decision-making behind AI development. Having to decide what facial expressions are subject to happy or neutral puts our own biases into the machine. Thus connecting back to the main ideas in Unmasking AI, reinforcing Buolamwini's message for the need for ethical decision-making in the AI development process. This project helped us all appreciate how AI systems learn and what must be integrated into the systems to help the models flourish. Building the machine showed us how easily models can misread human behavior and the importance of clear data training. </p> </p>
                <!-- PROJECT SCOPE HEADER -->
                <h2 id="title">Project Scope</h2>

                <p>
                    <!-- Basic scope info -->
                    Classification: Determining Different Emotions <br>
                    Range: 3 Emotions <br>
                    Emotions Listed: Happy, Neutral, Sad <br>
                    Image Samples: 200-300 images for each classified item <br>

                    <!-- Project Image -->
                    <img src="Machine Learning image.jpg" alt="Teachable Machine Screenshot" width="700" height="400">
                </p>

                <!-- REFLECTIONS SECTION -->
                <h2 id="title">Reflections On Our Project & Unmasking AI by Joy Bolumawani</h2>

                <!-- Reflection Paragraphs -->
              <p> Our machine learning technology teaches the computer how to recognize three basic facial expressions, accompanied by three basic hand motions: Happy, Neutral, and Sad, represented by thumbs up, middle, and down. By using dozens of pictures of various women, we trained the teachable machine model to distinguish between these faces. The lessons from Joy Buolamwini's Unmasking AI translate to practical and ethical decisions we had to make in the process. Despite our project being a scaled-back version of machine learning, the arguments Boulamwini makes on data, power, and representation were very evident throughout the process. ​</p>
            <p> We began by deciding which expressions to include. We chose happy, neutral, and sad because they are among the most commonly recognized emotional states and could be reliably demonstrated by any user. Once we finalized our categories, we each collected the photo data for each class using the Teachable Machine interface and even had our friends try it too. We quickly ran into the issue that this machine was having trouble accurately attributing the right facial expression to the correct emotion. So we decided to also hold a thumbs up with happiness, a thumb in the middle with neutral, and a thumbs down with sadness. Adding a hand signal easily allowed the machine to identify the correct emotion. When creating this machine learning technology, we found ourselves running into many of the same issues Buolamwini raises around bias when building technology. </p>   
            <p> One concept from the book that we discussed through the creation of our machine learning tool was the idea of “Coded Gaze.” This term explains how people who design and train these AI systems will subconsciously and unintentionally pass personal preferences or experiences onto the technology. As we consciously took repetitive photos, we realized that we were the ones who decided what counted as happy, neutral, or sad. We were training this model to interpret our emotions through our lenses. Thus, proving to us Boulamwini's point that these machines become biased based on what they learn from limited human perspectives. </p>    
               <p> As Boulamwini discussed in her book, AI systems are never neutral, as every classification reflects a human decision. When building our model, we first wanted the machine to identify emotions just based on our faces. However, when we tested, we found that neutral expressions were extremely hard for the model to recognize. This is because a neutral expression is very subjective. What each of our group members considered neutral emotion was not consistent. This showed us firsthand what Buolamwini expresses when she says that these systems are classified and shaped by human perspectives. </p>
              <p> As we continued to test our model, we saw that the system confidently misclassified subtle expression differences. Highlighting the gap between intent and impact. Showing that many harmful AI outcomes occur because the system misreads or misinterprets the images. While these mistakes were harmless for us, they showed how easy it is for AI to make simple mistakes that could negatively affect users.</p>
                <p> Building this small version of a machine learning tool prompted us to think critically about the ethical principles that support AI development. This tool showed us the importance of ethical reflection in every stage of AI development. This project directly translated the themes from Unmasking AI into our own practice. This project opened our eyes to the small details that go into machine learning and the impact small design choices have on the design.</p>
                <!-- MODEL VIDEO EXAMPLE SECTION -->
                <h2 id="title">Model Video Example</h2>

                <p>
                    <!-- Video Embed -->
                    <video width="640" height="360" controls>
                        <source src="Trainable Machine Video2.mp4" type="video/mp4">
                        <!-- Fallback text for unsupported browsers -->
                        Your browser does not support the video tag.
                    </video>
                </p>

                <!-- LINK TO EXTERNAL MODEL -->
                <h2 id="title">Test our Model!</h2>

                <p>
                    <!-- Clickable link to Teachable Machine -->
                    <a href="https://teachablemachine.withgoogle.com/models/j9Xe3_1yG/">
                        Our Model
                    </a>
                </p>

            </article>
        </main>
    </section>

</body>
</html>
