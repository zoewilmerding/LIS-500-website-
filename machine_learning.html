<!DOCTYPE html>
<html>
	<head> 
		<title>Machine Learning</title>
		<link rel="stylesheet" text="text/css" href="stylesheet2.css">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@500;700&display=swap" rel="stylesheet">

</head>

	<body>
		<header> 
		<h1>Machine Learning </h1>
		<nav> 
			<ul>
				<li><a href="index.html">Home</a></li> 
				<li><a href="about.html">About us</a></li> 
				<li><a href="resources.html">Resources</a></li>
				<li><a href="tech_hero.html">Tech Hero</a></li>
       		 	<li><a href="machine_learning.html">Machine Learning</a></li>
			</ul>
		</nav>
</header>
<section>
	<main>
		<article class="mart">
			<h2 id = "title"> Project Objectives  </h2>
			
			<p>  Our goal for the machine learning project was to understand first-hand how machine learning models are built, trained, and how design choices impact the accuracy of the systems. To do this, we created a teachable machine qualifier that identified between three basic facial expressions: happy, neutral, and sad. Through the design process, we were constantly pulling themes from this semester's reading of Unmasking AI, focusing on the importance of ethical decision-making. 
After running into issues with the machine classifying the "neutral" emotion, we decided to incorporate clear, visible hand signals. This addition simplified the recognition task and showed us the importance of detail for machine learning to interpret human behavior. We decided to have Amanda take all the pictures for each of the three emotions so the facial expressions and hand signals were as consistent as possible across the hundred photos. This kept the lighting, angle, distance from the camera, and camera quality all the same across the images. We quickly learned that the more photos we had of each expression, the easier it is for the machine to correctly identify the emotion. After realizing how many pictures the machine needs to function well, it showed us the importance of detail in machine learning and in the training data.
	We repeated the design cycle of collect, train, test, and adjust over and over until the machine was working just how we liked it. As a group, we made conscious decisions about which images to keep and made sure that the images reflected the emotions we were trying to portray. These decisions we made as a group shaped how the machine learns to interpret human emotion.
When designing our model we quickly found that a neutral expression was nearly impossible for the machine to accurately identify. We found that it was hard for Amanda to display neutrality consistently for the machine to recognize correctly. Some shots looked more serious, leaned towards happy, or appeared as sad. While with practice of taking many photos and adding the hand motion we were able to overcome this issue we were constantly referring back to Buolamwi's core themes. As she explains in her book AI systems take on the subjective assumptions in their training.  Our model reinforced this argument that these teachable machines reflect the interpretations of the ones creating. 
	The process of creating the model made us extremely aware of the ethical decision-making behind AI development. Having to decide what facial expressions  are subject to happy or neutral puts our own biases into the machine. Thus connecting back to the main ideas in Unmasking AI, reinforcing Buolamwini's message for the need for ethical decision-making in the AI development process. This project helped us all appreciate how AI systems learn and what must be integrated into the systems to help the models flourish. Building the machine showed us how easily models can misread human behavior and the importance of clear data training. 


</p>
			
			<h2 id = "title"> Project Scope </h2>
			
			<p> 
			Classification: Determining Different Emotions <br>
			Range: 3 Emotions <br>
			Emotion's Listed: Happy , Neutral , Sad <br>
			Image Samples: 60-80 images for each classified item <br>
			<img src="PNG image.jpeg" alt="IMG" width="500" height="400" />		
			
			</p>
			
			<h2 id = "title"> Reflections On Our Project & Unmasking AI by Joy Bolumawani </h2>
			
			<p>  Our machine learning technology teaches the computer how to recognize three basic facial expressions, accompanied by three basic hand motions: Happy, Neutral, and Sad, represented by thumbs up, middle, and down. By using dozens of pictures of Amanda recreating these facial features, we trained the teachable machine model to distinguish between these faces. The lessons from Joy Buolamwini's Unmasking AI translate to practical and ethical decisions we had to make in the process. Despite our project being a scaled-back version of machine learning, the arguments Boulamwini makes on data, power, and representation were very evident throughout the process. ​
We began by deciding which expressions to include. We chose happy, neutral, and sad because they are among the most commonly recognized emotional states and could be reliably demonstrated by any user. Once we finalized our categories, Amanda collected the photo data for each class using the Teachable Machine interface. We quickly ran into the issue that this machine was having trouble accurately attributing the right facial expression to the correct emotion. So we decided to also hold a thumbs up with happiness,  a thumb in the middle with neutral, and a thumbs down with sadness. Adding a hand signal easily allowed the machine to identify the correct emotion. When creating this machine learning technology, we found ourselves running into many of the same issues Buolamwini raises around bias when building technology. 
One concept from the book that we discussed through the creation of our machine learning tool was the idea of “Coded Gaze.” This term explains how people who design and train these AI systems will subconsciously and unintentionally pass personal preferences or experiences onto the technology. As we consciously took repetitive photos, we realized that we were the ones who decided what counted as happy, neutral, or sad. We were training this model to interpret our emotions through our lenses. Thus, proving to us Boulamwini's point that these machines become biased based on what they learn from limited human perspectives. 
As Boulamwini discussed in her book, AI systems are never neutral, as every classification reflects a human decision. When building our model, we first wanted the machine to identify emotions just based on our faces. However, when we tested, we found that neutral expressions were extremely hard for the model to recognize.  This is because a neutral expression is very subjective. What each of our group members considered neutral emotion was not consistent. This showed us firsthand what Buolamwini expresses when she says that these systems are classified and shaped by human perspectives. 
As we continued to test our model, we saw that the system confidently misclassified subtle expression differences. Highlighting the gap between intent and impact. Showing that many harmful AI outcomes occur because the system misreads or misinterprets the images. While these mistakes were harmless for us, they showed how easy it is for AI to make simple mistakes that could negatively affect users.
Building this small version of a machine learning tool prompted us to think critically about the ethical principles that support AI development. This tool showed us the importance of ethical reflection in every stage of AI development. This project directly translated the themes from Unmasking AI into our own practice. This project opened our eyes to the small details that go into machine learning and the impact small design choices have on the design.
</p>
			
			<h2 id = "title"> Model Video Example  </h2>
			
			<p> 
				<video width="640" height="360" controls>
    <source src="Teachable Machine.mp4" type="video/mp4">
    </video>
				
   			</p>
			<h2 id = "title"> Test our Model! </h2>
			
			<p>  <a href="https://teachablemachine.withgoogle.com/models/GWs1-9pCX/"> Our Model </a> </p>
		</article>
</section>

	</body>

</html>
